{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Recurrent Network (VRNN)\n",
    "\n",
    "Implementation based on Chung's *A Recurrent Latent Variable Model for Sequential Data* [arXiv:1506.02216v6].\n",
    "\n",
    "##  Network design\n",
    "\n",
    "\n",
    "\n",
    "There are three types of layers: input (x), hidden(h) and latent(z). We can compare VRNN sided by side with RNN to see how it works in generation phase.\n",
    "\n",
    "\n",
    "- RNN: $h_o + x_o -> h_1 + x_1 -> h_2 + x_2 -> ...$\n",
    "- VRNN: with $ h_o \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      h_o -> z_1 \\\\\n",
    "      z_1 + h_o -> x_1\\\\\n",
    "      z_1 + x_1 + h_o -> h_1 \\\\\n",
    "\\end{array} \n",
    "\\right .$ \n",
    "with $ h_1 \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      h_1 -> z_2 \\\\\n",
    "      z_2 + h_1 -> x_2\\\\\n",
    "      z_2 + x_2 + h_1 -> h_2 \\\\\n",
    "\\end{array} \n",
    "\\right .$\n",
    "\n",
    "\n",
    "It is clearer to see how it works in the code blocks below. This loop is used to generate new text when the network is properly trained. x is wanted output, h is deterministic hidden state, and z is latent state (stochastic hidden state). Both h and z are changing with repect to time.\n",
    "\n",
    "## Training\n",
    "\n",
    "\n",
    "\n",
    "The VRNN above contains three components, a latent layer genreator $h_o -> z_1$, a decoder net to get $x_1$, and a recurrent net to get $h_1$ for the next cycle.\n",
    "\n",
    "\n",
    "The training objective is to make sure $x_0$ is realistic. To do that, an encoder layer is added to transform $x_1 + h_0 -> z_1$. Then the decoder should transform $z_1 + h_o -> x_1$ correctly. This implies a cross-entropy loss in the \"tiny shakespear\" or MSE in image reconstruction.\n",
    "\n",
    "\n",
    "Another loose end is  $h_o -> z_1$. Statistically, $x_1 + h_0 -> z_1$ should be the same as $h_o -> z_1$, if $x_1$ is sampled randomly. This constraint is formularize as a KL divergence between the two.\n",
    "\n",
    "\n",
    "\n",
    ">### KL Divergence of Multivariate Normal Distribution\n",
    ">![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8dad333d8c5fc46358036ced5ab8e5d22bae708c)\n",
    "\n",
    "\n",
    "Now putting everything together for one training cycle.\n",
    "\n",
    "$\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      h_o -> z_{1,prior} \\\\\n",
    "      x_1 + h_o -> z_{1,infer}\\\\\n",
    "      z_1 <- sampling N(z_{1,infer})\\\\\n",
    "      z_1 + h_o -> x_{1,reconstruct}\\\\\n",
    "      z_1 + x_1 + h_o -> h_1 \\\\\n",
    "\\end{array} \n",
    "\\right . $\n",
    "=>\n",
    "$\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      loss\\_latent = DL(z_{1,infer} | z_{1,prior}) \\\\\n",
    "      loss\\_reconstruct = x_1 - x_{1,reconstruct} \\\\\n",
    "\\end{array} \n",
    "\\right .\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'w,\\x0c+4\\x08\\x11^IPc\\x1b\\x048\\x13,qIRG\\x0bdo\\x03~;BR681F}L\\x00\\x18{#xc3<E\\x02ud8<\\x07~\\x08*\\n}\\x1b\\x1eSN\\x191$s\\x009Lp{41QvE\\x04*cl~67\\x13\\x19N{y{HO{d]}\\rIz\"Y\\x0c\\x1c\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class VRNNCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VRNNCell,self).__init__()\n",
    "        self.phi_x = nn.Sequential(nn.Embedding(128,64), nn.Linear(64,64), nn.ELU())\n",
    "        self.encoder = nn.Linear(128,64*2) # output hyperparameters\n",
    "        self.phi_z = nn.Sequential(nn.Linear(64,64), nn.ELU())\n",
    "        self.decoder = nn.Linear(128,128) # logits\n",
    "        self.prior = nn.Linear(64,64*2) # output hyperparameters\n",
    "        self.rnn = nn.GRUCell(128,64)\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.phi_x(x)\n",
    "        # 1. h => z\n",
    "        z_prior = self.prior(hidden)\n",
    "        # 2. x + h => z\n",
    "        z_infer = self.encoder(torch.cat([x,hidden], dim=1))\n",
    "        # sampling\n",
    "        z = Variable(torch.randn(x.size(0),64))*z_infer[:,64:].exp()+z_infer[:,:64]\n",
    "        z = self.phi_z(z)\n",
    "        # 3. h + z => x\n",
    "        x_out = self.decoder(torch.cat([hidden, z], dim=1))\n",
    "        # 4. x + z => h\n",
    "        hidden_next = self.rnn(torch.cat([x,z], dim=1),hidden)\n",
    "        return x_out, hidden_next, z_prior, z_infer\n",
    "    def calculate_loss(self, x, hidden):\n",
    "        x_out, hidden_next, z_prior, z_infer = self.forward(x, hidden)\n",
    "        # 1. logistic regression loss\n",
    "        loss1 = nn.functional.cross_entropy(x_out, x) \n",
    "        # 2. KL Divergence between Multivariate Gaussian\n",
    "        mu_infer, log_sigma_infer = z_infer[:,:64], z_infer[:,64:]\n",
    "        mu_prior, log_sigma_prior = z_prior[:,:64], z_prior[:,64:]\n",
    "        loss2 = (2*(log_sigma_infer-log_sigma_prior)).exp() \\\n",
    "                + ((mu_infer-mu_prior)/log_sigma_prior.exp())**2 \\\n",
    "                - 2*(log_sigma_infer-log_sigma_prior) - 1\n",
    "        loss2 = 0.5*loss2.sum(dim=1).mean()\n",
    "        return loss1, loss2, hidden_next\n",
    "    def generate(self, hidden=None, temperature=None):\n",
    "        if hidden is None:\n",
    "            hidden=Variable(torch.zeros(1,64))\n",
    "        if temperature is None:\n",
    "            temperature = 0.8\n",
    "        # 1. h => z\n",
    "        z_prior = self.prior(hidden)\n",
    "        # sampling\n",
    "        z = Variable(torch.randn(z_prior.size(0),64))*z_prior[:,64:].exp()+z_prior[:,:64]\n",
    "        z = self.phi_z(z)\n",
    "        # 2. h + z => x\n",
    "        x_out = self.decoder(torch.cat([hidden, z], dim=1))\n",
    "        # sampling\n",
    "        x_sample = x = x_out.div(temperature).exp().multinomial(1).squeeze()\n",
    "        x = self.phi_x(x)\n",
    "        # 3. x + z => h\n",
    "        xkl = x.view(1,-1)\n",
    "        hidden_next = self.rnn(torch.cat([xkl,z], dim=1),hidden)\n",
    "        return x_sample, hidden_next\n",
    "    def generate_text(self, hidden=None,temperature=None, n=100):\n",
    "        res = []\n",
    "        hidden = None\n",
    "        for _ in range(n):\n",
    "            x_sample, hidden = self.generate(hidden,temperature)\n",
    "            res.append(chr(x_sample.data[0]))\n",
    "        return \"\".join(res)\n",
    "        \n",
    "# Test\n",
    "net = VRNNCell()\n",
    "x = Variable(torch.LongTensor([12,13,14]))\n",
    "hidden = Variable(torch.rand(3,64))\n",
    "output, hidden_next, z_infer, z_prior = net(x, hidden)\n",
    "loss1, loss2, _ = net.calculate_loss(x, hidden)\n",
    "loss1, loss2\n",
    "hidden = Variable(torch.zeros(1,64))\n",
    "net.generate_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Download tiny shakspear text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----SAMPLE----\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "from six.moves.urllib import request\n",
    "url = \"https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt\"\n",
    "text = request.urlopen(url).read().decode()\n",
    "\n",
    "print('-----SAMPLE----\\n')\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A convinient function to sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3c68f1e29e5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3c68f1e29e5e>\u001b[0m in \u001b[0;36mbatch_generator\u001b[0;34m(seq_size, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mseq_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_generator(seq_size=300, batch_size=64):\n",
    "    cap = len(text) - seq_size*batch_size\n",
    "    while True:\n",
    "        idx = np.random.randint(0, cap, batch_size)\n",
    "        res = []\n",
    "        for _ in range(seq_size):\n",
    "            batch = torch.LongTensor([ord(text[i]) for i in idx])\n",
    "            res.append(batch)\n",
    "            idx += 1\n",
    "        yield res\n",
    "\n",
    "g = batch_generator()\n",
    "batch = next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch 0, loss    4114.2339, decoder loss    1460.9972, latent loss    2653.2354\n",
      "@{k>\u0004VP^\u001e",
      "\u001f*\f",
      "\u0011V T~\u0015=|!pQQQrp[F{\u0018\u001b\u0007$~r\u001aN\u0001\u0005^xI.\u001c",
      "=c\u0018/fU\u0002/\u00131m Vi\"2nD\u00038oYg\\4(0\n",
      "\n",
      ">> epoch 200, loss     634.5780, decoder loss     627.3054, latent loss       7.2730\n",
      "thald thy a tre.\n",
      "\n",
      "BOMIPIO:\n",
      "Thame diour as bear? I howaw the theath!\n",
      "\n",
      "LONVENIO:\n",
      "I' as of in mor elipl\n",
      "\n",
      ">> epoch 400, loss     543.5396, decoder loss     540.6561, latent loss       2.8834\n",
      "ing Some!\n",
      "'To dear stoul I goes to past\n",
      "Seepese the sore of foul with my doust.\n",
      "Serveren that preato\n",
      "\n",
      ">> epoch 600, loss     515.4258, decoder loss     512.3687, latent loss       3.0574\n",
      "BOLESSER:\n",
      "Which you are and thy hast at a grovesed and\n",
      "To his grave for my franted have ance time\n",
      "Th\n",
      "\n",
      ">> epoch 800, loss     500.3734, decoder loss     497.9365, latent loss       2.4368\n",
      "tent and then report,\n",
      "When you sale none be of accuce\n",
      "The dukes the day of Withter us with look.\n",
      "\n",
      "FR\n",
      "\n",
      ">> epoch 1000, loss     487.2025, decoder loss     483.1525, latent loss       4.0500\n",
      "re dis men wome we courticace,\n",
      "For nothing one with which his house:\n",
      "That I mean with you be sommast\n",
      "\n",
      ">> epoch 1200, loss     484.1796, decoder loss     480.3503, latent loss       3.8291\n",
      "an,\n",
      "The stone like he the quick the good vares and of with.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Hell's not boy, which b\n",
      "\n",
      ">> epoch 1400, loss     471.9077, decoder loss     468.8736, latent loss       3.0341\n",
      "iters of his browns,\n",
      "Let this brother to despage the rape and now:\n",
      "And not proud love that the footh\n",
      "\n",
      ">> epoch 1600, loss     488.8728, decoder loss     485.3548, latent loss       3.5179\n",
      "BUCHIO:\n",
      "Her, he seeming to their man brown be like my\n",
      "Forthe burst with mottering infair to the hear\n",
      "\n",
      ">> epoch 1800, loss     468.6732, decoder loss     466.0216, latent loss       2.6515\n",
      "dain,\n",
      "Was as nothing both your lives the vister'd\n",
      "And should such her thou ere come of the name's ga\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sal/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type VRNNCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "net = VRNNCell()\n",
    "# the_model = torch.load(model_path)\n",
    "max_epoch = 2000\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "g = batch_generator()\n",
    "model_path = \"save/model.pt\"\n",
    "\n",
    "\n",
    "hidden = Variable(torch.zeros(64,64)) #batch_size x hidden_size\n",
    "for epoch in range(max_epoch):\n",
    "    batch = next(g)\n",
    "    loss_seq = 0\n",
    "    loss1_seq, loss2_seq = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "    for x in batch:\n",
    "        loss1, loss2, hidden = net.calculate_loss(Variable(x),hidden)\n",
    "        loss1_seq += loss1.data[0]\n",
    "        loss2_seq += loss2.data[0]\n",
    "        loss_seq = loss_seq + loss1+loss2\n",
    "    loss_seq.backward()\n",
    "    optimizer.step()\n",
    "    hidden.detach_()\n",
    "    if epoch%200==0:\n",
    "        print('>> epoch {}, loss {:12.4f}, decoder loss {:12.4f}, latent loss {:12.4f}'.format(epoch, loss_seq.data[0], loss1_seq, loss2_seq))\n",
    "        print(net.generate_text())\n",
    "        print()\n",
    "torch.save(net, model_path)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a each as:\n",
      "Of my lord untemb, last do well up,\n",
      "Well, and stalm not.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Mark to more too, women te love is a dearte:\n",
      "Hake you, Mancia; that Bonenday, one fortun\n",
      "To spirly -troeable, there injul'd the darchicly capsent\n",
      "vice, best viciunties, soon soule\n",
      "Duke these then is not break itoar on our treat\n",
      "And that we't him into her sea-mend, thy queen I in\n",
      "This hearth beasugnes fellow. Wh' lords to thyself?\n",
      "My sword mascholeny, to wear your late,\n",
      "Provershing goest luins in thy genize hearth.\n",
      "\n",
      "Second Roman,\n",
      "He's behonan may grace of her and reason on their siltry,\n",
      "And so, and breaks a hundey, give your hopety fall\n",
      "And cruidy love, mayself bevil in that,\n",
      "With Tripst Auforable too peatue,\n",
      "Shall thou dranch in eye--nown, this?\n",
      "\n",
      "First JEWIS:\n",
      "Will go I do you entruct seem;\n",
      "From the comastice accondily imprast:\n",
      "Be stand yet, if a hout by thy pities\n",
      "Tell? beings, being va generalt couse,\n",
      "I,\n",
      "she duke weep to our lame is in York, there up the gentless?\n",
      "\n",
      "RICHARD:\n",
      "A knee he but ro'd friend, sosh\n"
     ]
    }
   ],
   "source": [
    "model_path = \"save/model.pt\"\n",
    "cdss = torch.load(model_path)\n",
    "sample = cdss.generate_text(n=1000, temperature=1)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "- Denifinitely train longer to get better results. \n",
    "- Keep in mind the rnn kernel only has 1 layer, with 64 neurons.\n",
    "- Seems no need to tune temperature here. temperature = 0.8 generates a lot of obscure spelling. temperature = 1 works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  app.launch_new_instance()\n",
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/sal/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch 0, loss     464.4120, decoder loss     464.2681, latent loss       0.1439\n",
      "and,\n",
      "I shall be meer it of you in the speak.\n",
      "\n",
      "GLOUCESTER:\n",
      "Thine take me my lord, but it is scarned t\n",
      "\n",
      ">> epoch 20, loss     463.3658, decoder loss     463.2422, latent loss       0.1236\n",
      " not not that his hage:\n",
      "My peiconds, so wite thee come to the breest;\n",
      "Remour else, him say the purtu\n",
      "\n",
      ">> epoch 40, loss     462.0584, decoder loss     461.9486, latent loss       0.1099\n",
      "ites:\n",
      "From Henry, this mounth gods and Petual:\n",
      "O, I sige suppose; wate all thine Rome,\n",
      "And the fair \n",
      "\n",
      ">> epoch 60, loss     452.8532, decoder loss     452.7492, latent loss       0.1040\n",
      "ar,\n",
      "That we have duke, but another, God's stand\n",
      "how, and give on on our jeam and first;\n",
      "Pert in the \n",
      "\n",
      ">> epoch 80, loss     447.1253, decoder loss     447.0273, latent loss       0.0979\n",
      "seth!\n",
      "\n",
      "AUFIDIUS:\n",
      "It brauds is thy flien, I pady more for the deapetts,\n",
      "And the call to be instartine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"save/model.pt\"\n",
    "net = torch.load(model_path)\n",
    "max_epoch = 100\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "g = batch_generator()\n",
    "\n",
    "\n",
    "hidden = Variable(torch.zeros(64,64)) #batch_size x hidden_size\n",
    "for epoch in range(max_epoch):\n",
    "    batch = next(g)\n",
    "    loss_seq = 0\n",
    "    loss1_seq, loss2_seq = 0, 0\n",
    "    optimizer.zero_grad()\n",
    "    for x in batch:\n",
    "        loss1, loss2, hidden = net.calculate_loss(Variable(x),hidden)\n",
    "        loss1_seq += loss1.data[0]\n",
    "        loss2_seq += loss2.data[0]\n",
    "        loss_seq = loss_seq + loss1+loss2\n",
    "    loss_seq.backward()\n",
    "    optimizer.step()\n",
    "    hidden.detach_()\n",
    "    if epoch%20==0:\n",
    "        print('>> epoch {}, loss {:12.4f}, decoder loss {:12.4f}, latent loss {:12.4f}'.format(epoch, loss_seq.data[0], loss1_seq, loss2_seq))\n",
    "        print(net.generate_text())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sal/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type VRNNCell. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, model_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
